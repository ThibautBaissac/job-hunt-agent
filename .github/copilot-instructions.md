# Copilot Instructions
- **Repo layout**: Monorepo with Rails UI/business logic in `rails_app` and Python microservices in `python_services`; reference product docs under `docs/` before adding features.
- **Domain context**: The canonical requirements live in `docs/specs fonctionnelles.md` and supporting architecture notes in `docs/architecture rails.md` and `docs/architecture repo.md`; new work should stay faithful to these flows (offer import → AI prep → user validation → Kanban tracking).
- **Rails stack**: Rails 8.1 + importmap, Turbo, and Stimulus; keep browser JS in `rails_app/app/javascript`, register new modules in `rails_app/config/importmap.rb`, and avoid bundler-specific code.
- **Data model focus**: Core models are User/Profile, Cv, JobOffer, Application, ApplicationNote, EmailTemplate, SentEmail; align associations, enums, and validations with the breakdown in `docs/architecture rails.md`.
- **Service objects**: Business logic is meant to live in POROs (namespaces like `OfferImporters`, `Ai`, `Integrations`, `ApplicationWorkflow`); prefer adding Ruby service classes over fat controllers or models.
- **HTTP integrations**: Rails talks to Python services via JSON over HTTP using Faraday-style clients (see expectations in `docs/architecture repo.md`); load base URLs from `.env` keys `AGENT_API_URL` and `SCRAPER_API_URL` through `dotenv-rails`.
- **AI pipeline contract**: When Rails calls the agent service, expect payload `{ job_offer: {...}, cv: {...}, profile: {...}, template: {...} }` and response fields like `summary`, `match_score`, `email_subject`, `email_body`, `cover_letter`, `cv_suggestions`; keep these shapes stable so the UI can bind to them.
- **Scraper contract**: Scraper service receives `{ "url": "..." }` and returns `{ title, company, location, description, platform }`; handle errors/timeouts explicitly and surface actionable messages in Rails.
- **Python structure**: `python_services/` contains `agent_api/` and `scraper_api/` (both FastAPI apps on ports 8001/8002), plus `api/` which holds shared dependencies (`requirements.txt`, `.venv`, `pyproject.toml`); both services import from their root-level packages and share the same virtual environment.
- **Python service layout**: Each service has `main.py` at its root defining the FastAPI app, plus nested packages: `agent_api/routers/`, `agent_api/core/` for LangChain chains/tools, and `scraper_api/parsers/` for platform-specific scrapers; keep service-specific code isolated within each service directory.
- **Python dependencies**: All Python dependencies live in `python_services/api/requirements.txt` and are installed in `python_services/api/.venv`; use Python 3.13+ compatible versions (FastAPI >=0.115, Pydantic >=2.10, LangChain >=0.3, Playwright >=1.48); run `.venv/bin/playwright install` after pip install.
- **LangChain usage**: Keep chains, prompts, and tool integrations inside `python_services/agent_api/core/`; isolate prompt wording so Rails changes do not require Python edits.
- **Playwright usage**: Place site-specific selectors under `python_services/scraper_api/parsers/` and keep browser automation deterministic; add new platforms as separate parser modules; browsers are installed system-wide via `playwright install`.
- **Environment setup**: Use the root `.env` (not committed) to provide Rails env, Python URLs (`AGENT_API_URL=http://localhost:8001`, `SCRAPER_API_URL=http://localhost:8002`), OpenAI, and Google OAuth secrets; never hardcode secrets in code or fixtures.
- **Local dev commands**: From repo root, run `bin/dev` to start all services (Rails on :5000, agent_api on :8001, scraper_api on :8002) via foreman; from `rails_app/`, run `bin/setup --skip-server` once, `bin/jobs` to process Solid Queue jobs, and `bin/rails db:prepare` before tests or migrations.
- **Testing pipeline**: Use `bin/ci` for the full suite (Rubocop, bundler audit, brakeman, Rails tests, system tests, seed replant). For quick checks, `bin/rails test` and `bin/rails test:system` are available.
- **Background jobs**: Solid Queue/Cache/Cable split databases are configured via `config/database.yml` and schema files under `db/*_schema.rb`; ensure migrations respect this separation.
- **Email delivery**: Gmail integration should go through `Integrations::GmailClient`, storing delivery metadata in `SentEmail`; keep OAuth token handling isolated and auditable.
- **Kanban workflow**: Application status drives the board states (`to_process`, `preparing`, `sent`, `interview`, `offer`, `rejected`, `no_answer`); when mutating status, also update timestamps like `last_status_change_at` and record notes where relevant.
- **UI binding**: Views live under `rails_app/app/views`; prefer Stimulus controllers for interactive elements and ensure Turbo-compatible responses.
- **Seeds & fixtures**: `db/seeds.rb` should create coherent demo data spanning offers, applications, and CVs so the AI workflows can be exercised end-to-end.
- **Process orchestration**: Run `bin/dev` from repo root to start all services via Procfile.dev (rails, agent_api, scraper_api); optional `docker-compose` with services `rails_app`, `agent_api`, `scraper_api`, and `postgres` can be added later if needed.
- **Error handling**: Bubble API failures with clear context (HTTP status, body snippet) and fall back to manual input rather than crashing the Rails request cycle.
- **Extensibility mindset**: Each step (offer import, analysis, matching, content generation, send) should stay modular so automation can expand later; avoid cross-coupling service responsibilities.
- **Documentation**: Whenever you introduce new workflows or data contracts, update the relevant file in `docs/` and, if API-facing, note the request/response examples.
- **Coding style**: Prefer idiomatic Rails/ActiveRecord patterns, lean view partials, and test coverage that mirrors the documented use cases (offer import, application preparation, Gmail send).
